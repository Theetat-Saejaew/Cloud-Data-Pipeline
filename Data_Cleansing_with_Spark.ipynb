{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Data Cleansing with Spark",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Theetat-Saejaew/Cloud-Data-Pipeline/blob/main/Data_Cleansing_with_Spark.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_SQc75fHqX85"
      },
      "source": [
        "# Data Cleansing with Spark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hZ06tCiTn7z9"
      },
      "source": [
        "# Step 1) ติดตั้ง Spark และ PySpark\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3SV-YTNRpMKy"
      },
      "source": [
        "!apt-get update                                                                          # อัพเดท Package ทั้งหมดใน VM ตัวนี้\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null                                  # ติดตั้ง Java Development Kit (จำเป็นสำหรับการติดตั้ง Spark)\n",
        "!wget -q https://archive.apache.org/dist/spark/spark-3.1.2/spark-3.1.2-bin-hadoop2.7.tgz # ติดตั้ง Spark 3.1.2\n",
        "!tar xzvf spark-3.1.2-bin-hadoop2.7.tgz                                                  # Unzip ไฟล์ Spark 3.1.2\n",
        "!pip install -q findspark==1.3.0                                                         # ติดตั้ง Package Python สำหรับเชื่อมต่อกับ Spark "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Dwpn8wv9lgO"
      },
      "source": [
        "# Set enviroment variable ให้ Python รู้จัก Spark\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.1.2-bin-hadoop2.7\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qHNIVIS8ygwc"
      },
      "source": [
        "# ติดตั้ง PySpark ลงใน Python\n",
        "!pip install pyspark==3.1.2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2aX_fiEkrkVo"
      },
      "source": [
        "## ใช้งาน Spark"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ASZqLPXDPJJB"
      },
      "source": [
        "# Server ของ Google Colab มีกี่ Core\n",
        "!cat /proc/cpuinfo"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XOC1AgcQwRz1"
      },
      "source": [
        "ใช้ `local[*]` เพื่อเปิดการใช้งานการประมวลผลแบบ multi-core (Spark จะใช้ CPU ทุก core ที่อนุญาตให้ใช้งานในเครื่อง)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e6-PYi44nH2e"
      },
      "source": [
        "# สร้าง Spark Session เพิ้อใช้งาน Spark\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c8Fuuh_8ocdw"
      },
      "source": [
        "# ดูเวอร์ชั่น Python\n",
        "import sys\n",
        "sys.version_info"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "obpcIPP4oOvk"
      },
      "source": [
        "# ดูเวอร์ชั่น Spark\n",
        "spark.version"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7t8xQWVEi7tU"
      },
      "source": [
        "## Load  Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "48EFzlQd-FET"
      },
      "source": [
        "คำอธิบายคำสั่ง:\n",
        "\n",
        "wget = คำสั่งในการดาวน์โหลดไฟล์\n",
        "\n",
        "wget -O = ตั้งชื่อไฟล์"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2a0K_ljDi-r5"
      },
      "source": [
        "# Download Data File\n",
        "!wget -O data.zip https://file.designil.com/zdOfUE+\n",
        "!unzip data.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Pn2zW6xqrtY"
      },
      "source": [
        "### Load data ใส่ Spark\n",
        "\n",
        "ใช้คำสั่ง `spark.read.csv` เพื่ออ่านข้อมูลจากไฟล์ CSV\n",
        "\n",
        "Arguments:\n",
        "\n",
        "Header = True << บอกให้ Spark รู้ว่าบรรทัดแรกในไฟล์ CSV เป็น Header\n",
        "\n",
        "Inferschema = True << บอกให้ Spark พยายามเดาว่าแต่ละ column มี type เป็นอะไร [ ถ้าตั้งเป็น False, ทุก column จะถูกอ่านเป็น string (ตัวหนังสือ) ]\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Ho4MvryoY9u"
      },
      "source": [
        "dt = spark.read.csv('/content/ws2_data.csv', header = True, inferSchema = True, )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gKBjHvpzTM6-"
      },
      "source": [
        "# Step 2) Data Profiling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ruTiAh69nkWT"
      },
      "source": [
        "Data Profiling เป็นการทำความเข้าใจข้อมูลเบื้องต้น เพื่อที่เราจะได้รู้ว่าข้อมูลนี้มีคอลัมน์ไหนบ้าง ค่าโดยรวมเป็นอย่างไรบ้าง ฯลฯ เพื่อให้เราตัดสินใจได้ต่อว่าจะเช็คที่จุดไหนต่อไป\n",
        "\n",
        "ตัวอย่าง: max, min, average, sum, มี missing value มั้ย ฯลฯ"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qpb7PcpSrOWe"
      },
      "source": [
        "# ดูว่ามีคอลัมน์อะไรบ้าง\n",
        "dt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qpp8rs5mrcjo"
      },
      "source": [
        "> Columns:\n",
        "1. timestamp\n",
        "2. user_id\n",
        "3. book_id\n",
        "4. country\n",
        "5. price"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7rEzUAWCS3Gd"
      },
      "source": [
        "# ดูข้อมูล\n",
        "dt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XOgygtSZUenr"
      },
      "source": [
        "# ดูข้อมูล 100 แถวแรก\n",
        "dt.show(100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-hfU4p2sCuef"
      },
      "source": [
        "# ดูประเภทข้อมูลแต่ละคอลัมน์\n",
        "dt.dtypes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Er9ltLyoCJxo"
      },
      "source": [
        "# อีกคำสั่งในการดูข้อมูลแต่ละคอลัมน์ (Schema)\n",
        "dt.printSchema()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uu34YbPnnCuL"
      },
      "source": [
        "nullable คือ ค่าสามารถเป็น null ได้\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "brQdwlRpCVSi"
      },
      "source": [
        "# นับจำนวนแถวและ column\n",
        "print((dt.count(), len(dt.columns)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hZ9V4iQKUi2i"
      },
      "source": [
        "# สรุปข้อมูลสถิติ\n",
        "dt.describe().show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DJ7xtdKgVvKO"
      },
      "source": [
        "# อีกคำสั่งในการสรุปข้อมูลสถิติ\n",
        "dt.summary().show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LBmjzCqjM_MR"
      },
      "source": [
        "# สรุปข้อมูลสถิติเฉพาะ column ที่ระบุ\n",
        "dt.select(\"price\").describe().show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hLnRRd2MrvAl"
      },
      "source": [
        "# คอลัมน์ไหนมี Missing Value บ้าง? และแสดงข้อมูลแถวที่มี Missing Value\n",
        "dt.summary('count').show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dt.where(dt.user_id.isNull()).show()"
      ],
      "metadata": {
        "id": "sem0hullpdRe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qe1VrTpETgar"
      },
      "source": [
        "# Step 3) EDA - Exploratory Data Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LRmig4MaQm2s"
      },
      "source": [
        "## Non-Graphical EDA\n",
        "\n",
        "เราสามารถใช้คำสั่ง Spark ในการค้นหาข้อมูลที่ต้องการได้"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AGkcJ3kDQmJ5"
      },
      "source": [
        "# ข้อมูลที่เป็นตัวเลข\n",
        "dt.where(dt.price >= 1).show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ccWQBqMXxd2O"
      },
      "source": [
        "# ข้อมูลที่เป็นตัวหนังสือ\n",
        "dt.where(dt.country == 'Canada').show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zgPm8dsraxNx"
      },
      "source": [
        "# การซื้อทั้งหมดที่เกิดขึ้นในเดือนเมษายน มีกี่แถว\n",
        "# การซื้อทั้งหมดที่เกิดขึ้นในเดือนสิงหาคม มีกี่แถว\n",
        "dt.where( dt.timestamp.startswith(\"2021-04\") ).count()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dt.where( dt.timestamp.startswith(\"2021-08\") ).count()"
      ],
      "metadata": {
        "id": "MAvupVDBrpuW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NN3m9gpHkS5H"
      },
      "source": [
        "## Graphical EDA\n",
        "\n",
        "\n",
        "Spark ไม่ได้ถูกพัฒนามาเพื่องาน plot ข้อมูล เพราะฉะนั้นเราจะใช้ package `seaborn` `matplotlib` และ `pandas` ในการ plot ข้อมูลแทน"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BWu_zxdONk0o"
      },
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "861ui21dNy-N"
      },
      "source": [
        "# แปลง Spark Dataframe เป็น Pandas Dataframe - ใช้เวลาประมาณ 6 วินาที spark จะใช้งานกับ bigdata ได้ดีกว่า\n",
        "dt_pd = dt.toPandas()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vpF73lu_Rn-_"
      },
      "source": [
        "# ดูตัวอย่างข้อมูล\n",
        "dt_pd.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dw91uylC7B_x"
      },
      "source": [
        "# Boxplot - แสดงการกระจายตัวของข้อมูลตัวเลข\n",
        "sns.boxplot(x = dt_pd['book_id'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IYbSz2HuO2sr"
      },
      "source": [
        "# Histogram - แสดงการกระจายตัวของข้อมูลตัวเลข\n",
        "# bins = จำนวน bar ที่ต้องการแสดง\n",
        "sns.histplot(dt_pd['price'], bins=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G-xTqiuGERU_"
      },
      "source": [
        "\n",
        "book_id เพิ่มขึ้นตามราคาหรือเปล่า?\n",
        "\n",
        "ลองสร้าง Plot เพื่อดูความสัมพันธ์ระหว่าง book_id กับ price"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t1N-03d9O83E"
      },
      "source": [
        "# book_id เพิ่มขึ้นตามราคาหรือเปล่า?\n",
        "# ลองสร้าง Plot เพื่อดูความสัมพันธ์ระหว่าง book_id กับ price\n",
        "sns.scatterplot(data=dt_pd, x='book_id', y='price')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tVliQZLC0LuP"
      },
      "source": [
        "#### Bonus: สร้าง interactive chart"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Bb0F-9zTI1X"
      },
      "source": [
        "# Plotly - interactive chart\n",
        "import plotly.express as px\n",
        "fig = px.scatter(dt_pd, 'book_id', 'price')\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fbAnv3_Rwj5l"
      },
      "source": [
        "# Step 4) Data Cleansing with Spark\n",
        "\n",
        "ทำความสะอาดข้อมูลด้วย Spark "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iXLHVsbwk08q"
      },
      "source": [
        "### แปลง Data Type\n",
        "\n",
        "ปัญหาที่เจอบ่อยที่สุดแบบหนึ่งในข้อมูล คือ **Data Type ไม่ตรงกับที่เราต้องการ**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pb7BB-e3k4g-"
      },
      "source": [
        "# Show top 5 rows\n",
        "dt.show(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f7gFtSr0Fbne"
      },
      "source": [
        "# Show Schema\n",
        "dt.printSchema()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pNVwy-FkHDrT"
      },
      "source": [
        "จะเห็นว่า `Timestamp` ถูกอ่านเป็นข้อมูลตัวหนังสือ (String) แต่เราอยากให้มันเป็นข้อมูลวันที่และเวลา (date time) จะทำยังไงดี?\n",
        "\n",
        "ก่อนอื่น เราต้องมาดูก่อนว่าคอลัมน์ Timestamp แสดงเลขวันที่ก่อน หรือเลขเดือนก่อน (DD/MM/YYYY หรือ MM/DD/YYYY)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hu6po7nVGhj6"
      },
      "source": [
        "dt.select(\"timestamp\").show(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s53J7rfXIvXH"
      },
      "source": [
        "เราจะมาใช้ฟังก์ชั่น to_timestamp ซึ่งอยู่ใน pyspark.sql.functions กัน"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rnerFBErFelC"
      },
      "source": [
        "# แปลง string เป็น datetime\n",
        "from pyspark.sql import functions as f\n",
        "\n",
        "dt_clean = dt.withColumn(\"timestamp\",\n",
        "                        f.to_timestamp(dt.timestamp, 'yyyy-MM-dd HH:mm:ss')\n",
        "                        )\n",
        "dt_clean.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pq7sQ-h6T6cp"
      },
      "source": [
        "dt_clean.printSchema()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YMAQ-mXsNf1z"
      },
      "source": [
        "## BONUS: ตัวอย่างการใช้ประโยชน์จากข้อมูล Datetime"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NMiWFpDtJFdM"
      },
      "source": [
        "# นับยอด transaction ช่วงครึ่งเดือนแรก ของเดือนมิถุนายน\n",
        "dt_clean.where( (f.dayofmonth(dt_clean.timestamp) <= 15) & ( f.month(dt_clean.timestamp) == 6 ) ).count()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NlewC3HjbMty"
      },
      "source": [
        "## Anomalies Check\n",
        "\n",
        "ใช้ Spark ตามหาสิ่งที่ผิดปกติในข้อมูล"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YQmzIi6c2Ugp"
      },
      "source": [
        "### ความผิดปกติ 1) Syntactical Anomalies\n",
        "**Lexical errors** เช่น สะกดผิด"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yKKS4l0-RUJE"
      },
      "source": [
        "\n",
        "\n",
        "หาชื่อประเทศที่สะกดผิด แล้วแก้ชื่อที่สะกดผิดให้ถูก"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "htfmeHuZ2VPo"
      },
      "source": [
        "# ใน Data set ชุดนี้ มีข้อมูลจากกี่ประเทศ\n",
        "dt_clean.select(\"country\").distinct().count()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K4Ei25HVQcC1"
      },
      "source": [
        "# แทนที่ ... ด้วยจำนวนประเทศ เพื่อดูรายชื่อประเทศทั้งหมด\n",
        "# sort = ทำให้ข้อมูลเรียงตามตัวอักษร อ่านง่ายขึ้น\n",
        "# show() ถ้าไม่ใส่ตัวเลขจะขึ้นมาแค่ 20 อัน และใส่ False เพื่อให้แสดงข้อมูลในคอลัมน์แบบเต็ม ๆ (หากไม่ใส่ คอลัมน์ที่ยาวจะถูกตัดตัวหนังสือ)\n",
        "dt_clean.select(\"Country\").distinct().sort(\"Country\").show( 58, False )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zE0Sxc2uRzNv"
      },
      "source": [
        "มาดูกันว่าประเทศที่ชื่อผิด มีข้อมูลหน้าตาเป็นอย่างไร"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FSGUoi-eUl5U"
      },
      "source": [
        "# เปลี่ยน ... เป็นชื่อประเทศที่คุณคิดว่าผิด\n",
        "dt_clean.where(dt_clean['Country'] == 'Japane').show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vx5pu23dR7kV"
      },
      "source": [
        "ได้เวลาลองเปลี่ยนชื่อประเทศให้สะกดถูกต้อง"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "03YDBKivjIfh"
      },
      "source": [
        "# เปลี่ยน ... เป็นชื่อประเทศที่คิดว่าผิด และ ... เป็นชื่อประเทศที่ถูกต้อง\n",
        "from pyspark.sql.functions import when\n",
        "\n",
        "dt_clean_country = dt_clean.withColumn(\"CountryUpdate\", when(dt_clean['Country'] == 'Japane', 'Japan').otherwise(dt_clean['Country']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aHp2xBfIknNx"
      },
      "source": [
        "# ตรวจสอบข้อมูลที่แก้ไขแล้ว\n",
        "dt_clean_country.select(\"CountryUpdate\").distinct().sort(\"CountryUpdate\").show(58, False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vkNf0x8oT6HB"
      },
      "source": [
        "# ดูหน้าตาข้อมูลตอนนี้\n",
        "dt_clean_country.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EXspKbmMlDEA"
      },
      "source": [
        "# เอาคอลัมน์ CountryUpdate ไปแทนที่คอลัมน์ Country\n",
        "dt_clean = dt_clean_country.drop(\"Country\").withColumnRenamed('CountryUpdate', 'Country')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1UP2l5g-VkFJ"
      },
      "source": [
        "# ดูหน้าตาข้อมูล\n",
        "dt_clean.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-kyrqLYF2YjO"
      },
      "source": [
        "### ความผิดปกติ 2) Semantic Anomalies\n",
        "\n",
        "**Integrity constraints**: ค่าอยู่นอกเหนือขอบเขตของค่าที่รับได้ เช่น\n",
        "- user_id: ค่าจะต้องเป็นตัวเลขหรือตัวหนังสือ 8 ตัวอักษร"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iasmydx2TEYG"
      },
      "source": [
        "# ดูว่าข้อมูล user_id ตอนนี้หน้าตาเป็นอย่างไร\n",
        "dt_clean.select(\"user_id\").show(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZVtj_wvpsdn4"
      },
      "source": [
        "# นับจำนวน user_id ทั้งหมด\n",
        "dt_clean.select(\"user_id\").count()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QzMCAOg5WlyX"
      },
      "source": [
        "\n",
        "\n",
        "หาว่า user_id ตรงตามรูปแบบที่เราต้องการมั้ย และแทนที่ด้วยค่าที่ใกล้เคียงถ้าไม่ตรง"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XaViXptRXJXQ"
      },
      "source": [
        "ดูว่า user_id ตรงตามรูปแบบที่เราต้องการ มีกี่แถว\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i3H6sWVsrSn-"
      },
      "source": [
        "# แทนที่ ... ด้วย Regular Expression ของรูปแบบ user_id ที่เราต้องการ\n",
        "# คำใบ้: ใน Regular Expression ที่เราต้องการ มี ^ นำหน้า และลงท้ายด้วย $\n",
        "dt_clean.where(dt_clean[\"user_id\"].rlike(\"^[a-z0-9]{8}$\")).count()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5tE-VcSQXWzp"
      },
      "source": [
        "# แทนที่ ... ด้วย Regular Expression ของรูปแบบ user_id ที่เราต้องการ\n",
        "dt_correct_userid = dt_clean.filter(dt_clean[\"user_id\"].rlike(\"^[a-z0-9]{8}$\"))\n",
        "dt_incorrect_userid = dt_clean.subtract(dt_correct_userid)\n",
        "\n",
        "dt_incorrect_userid.show(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "38ekkpo7XyDG"
      },
      "source": [
        "มาทำการแก้ไข user_id นี้ให้ถูกต้อง"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g2SAQYLuuKgX"
      },
      "source": [
        "# แทนค่าที่ผิด ด้วยค่าที่ถูกต้อง\n",
        "dt_clean_userid = dt_clean.withColumn(\"userid_update\",\n",
        "                                      when(dt_clean['user_id'] == 'ca86d17200' , 'ca86d172').otherwise(dt_clean['user_id'])\n",
        "                                      )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Ko7m6mbwJJj"
      },
      "source": [
        "# ตรวจสอบผลลัพธ์\n",
        "dt_correct_userid = dt_clean_userid.filter(dt_clean_userid[\"user_id\"].rlike(\"^[a-z0-9]{8}$\"))\n",
        "dt_incorrect_userid = dt_clean_userid.subtract(dt_correct_userid)\n",
        "\n",
        "dt_incorrect_userid.show(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m6LoL8etxNSO"
      },
      "source": [
        "# เอาคอลัมน์ user_id_update ไปแทนที่ user_id \n",
        "dt_clean = dt_clean_country.drop(\"user_id\").withColumnRenamed('userid_update', 'user_id')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4kCf3LsX2b8L"
      },
      "source": [
        "### ความผิดปกติ 3) Missing values\n",
        "\n",
        "การเช็คและแก้ไข Missing Values (หากจำเป็น)\n",
        "\n",
        "ค่า Missing Value คือ ค่าที่ว่างเปล่า\n",
        "\n",
        "เราจะรู้ได้ยังไงว่าคอลัมน์ไหนมีค่าว่างเปล่ากี่ค่า"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T11c7iUs24Cu"
      },
      "source": [
        "# วิธีที่ 1 ในการเช็ค Missing Value\n",
        "# เช่น [ print(i) for i in [1,2,3] ]\n",
        "\n",
        "# col = คำสั่ง Spark ในการเลือกคอลัมน์\n",
        "# sum = คำสั่ง Spark ในการคิดผลรวม\n",
        "from pyspark.sql.functions import col, sum\n",
        "\n",
        "dt_nulllist = dt_clean.select([ sum(col(colname).isNull().cast(\"int\")).alias(colname) for colname in dt_clean.columns ])\n",
        "dt_nulllist.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qH7jfJTZdFwk"
      },
      "source": [
        "# วิธีที่ 2 ในการเช็ค Missing Value - โค้ดสะอาดกว่ามาก แต่ต้องมาบวกลบเอง\n",
        "dt_clean.summary(\"count\").show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Bi3ql4PzXae"
      },
      "source": [
        "# ดูช้อมูลว่าแถวไหนมี user_id เป็นค่าว่างเปล่า \n",
        "\n",
        "dt_clean.where( dt_clean.user_id.isNull() ).show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xp3uwpf-Ty8Y"
      },
      "source": [
        "\n",
        "สมมติทางทีม Data Analyst แจ้งว่าอยากให้เราแทน user_id ที่เป็น NULL ด้วย 00000000 ไปเลย"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wnwxPdTHTyS5"
      },
      "source": [
        "# Answer here\n",
        "dt_clean_user_id = dt_clean.withColumn(\"userid_update\",\n",
        "                                      when(dt_clean.user_id.isNull() , '00000000').otherwise(dt_clean['user_id'])\n",
        "                                      )\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dt_clean = dt_clean_user_id.drop(\"user_id\").withColumnRenamed('userid_update', 'user_id')"
      ],
      "metadata": {
        "id": "TEw-DmBw89mq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eJUNqs_1e1SY"
      },
      "source": [
        "# เช็คว่า user ID ที่เป็น NULL หายไปแล้ว ?\n",
        "dt_clean.where( dt_clean.user_id.isNull() ).show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G9WmtPhUOJHi"
      },
      "source": [
        "### ความผิดปกติ 4) Outliers:\n",
        "\n",
        "ข้อมูลที่สูงหรือต่ำผิดปกติจากข้อมูลส่วนใหญ่\n",
        "\n",
        "มาลองใช้ Boxplot ในการหาค่า Outlier ของราคาหนังสือ"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hz_NYXoNfmBM"
      },
      "source": [
        "dt_clean_pd = dt_clean.toPandas()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ejSTm4SvfOeX"
      },
      "source": [
        "sns.boxplot(x = dt_clean_pd['price'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iI-MxExWfsXH"
      },
      "source": [
        "เห็นได้ว่ามีหนังสือบางเล่มที่ราคาสูงกว่าปกติไปเยอะมาก ลองมาดูกันว่าหนังสือ book_id อะไรบ้าง ที่ราคาเกิน $80"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qv--yXibgjal"
      },
      "source": [
        "dt_clean.where( dt_clean.price > 80 ).select(\"book_id\").distinct().show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rK7tOP_OgsOh"
      },
      "source": [
        "เราสามารถนำ Book_ID อันนี้ไปเช็คต่อกับแหล่งข้อมูลได้ ว่าเป็นหนังสืออะไร และราคาเกิน $80 ผิดปกติมั้ย\n",
        "\n",
        "ถ้าเอาไปเช็คในข้อมูลจาก Workshop 1 ก็จะพบว่า Book_ID = 635 คือ หนังสือชื่อ \"The Power Broker\"\n",
        "https://www.audible.com/pd/The-Power-Broker-Audiobook/B0051JH67K?ipRedirectOverride=true&overrideBaseCountry=true&pf_rd_p=2756bc30-e1e4-4174-bb22-bce00b971761&pf_rd_r=MF7KC1JQF3A6GK2ET8XM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tt7qEffui33t"
      },
      "source": [
        "![](https://file.designil.com/7h1WIp+)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JJij1Esri-r0"
      },
      "source": [
        "The Power Broker มีราคา $84 จริง และเป็นหนังสือเสียงที่มีความยาวถึง 66 ชั่วโมง\n",
        "\n",
        "**ในที่นี้ ถือว่าเป็น Outlier จริง แต่ไม่ได้เป็นข้อมูลที่ผิด จึงไม่ต้องแก้อะไร**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KMwaNBsSuygq"
      },
      "source": [
        "# แปลงข้อมูลจาก Spark DataFrame ให้เป็น TempView ก่อน\n",
        "dt.createOrReplaceTempView(\"data\")\n",
        "dt_sql = spark.sql(\"SELECT * FROM data\")\n",
        "dt_sql.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kQLzG7jz06vU"
      },
      "source": [
        "# ลองแปลงโค้ดสำหรับลิสต์ชื่อประเทศ Exercise 4 เป็น SQL\n",
        "dt_sql_country = spark.sql(\"\"\"\n",
        "SELECT distinct country\n",
        "FROM data\n",
        "ORDER BY country\n",
        "\"\"\")\n",
        "dt_sql_country.show(100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qFnsr05vYnOi"
      },
      "source": [
        "# ลองแปลงโค้ดสำหรับแทนที่ชื่อประเทศ จาก Exercise 4 เป็น SQL\n",
        "dt_sql_result = spark.sql(\"\"\"\n",
        "SELECT timestamp, user_id, book_id,\n",
        "  CASE WHEN country = 'Japane' THEN 'Japan' ELSE country END AS country,\n",
        "price\n",
        "FROM data\n",
        "\"\"\")\n",
        "dt_sql_result.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z4dAJncSmK1g"
      },
      "source": [
        "# เช็คผลลัพธ์ว่าถูกจริงมั้ย\n",
        "dt_sql_result.select(\"country\").distinct().sort(\"country\").show(58, False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2QHlrKnPgMRh"
      },
      "source": [
        "# RLIKE ใน SQL เพื่อตรวจเช็ครูปแบบ Regular Expression ได้\n",
        "dt_sql_result = spark.sql(\"\"\"\n",
        "SELECT *\n",
        "FROM data\n",
        "WHERE user_id NOT RLIKE \"^[a-z0-9]{8}$\"\n",
        "\"\"\")\n",
        "dt_sql_result.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cgCm2SiKgMP7"
      },
      "source": [
        "dt_sql_uid_result = spark.sql(\"\"\"\n",
        "SELECT timestamp, \n",
        "CASE WHEN user_id = 'ca86d17200' THEN 'ca86d172' ELSE user_id END AS user_id, \n",
        "book_id, country, price\n",
        "FROM data\n",
        "\"\"\")\n",
        "dt_sql_uid_result.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GZUucKv1q8hB"
      },
      "source": [
        "# เช็คว่าข้อมูลที่ผิด หายไปหรือยัง\n",
        "dt_sql_uid_result.where( dt_sql_uid_result.user_id == 'ca86d17200' ).show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lu3WQ4g8UBra"
      },
      "source": [
        "# Step 5) Save data เป็น CSV\n",
        "\n",
        "โดยปกติแล้ว Spark จะทำการ Save ออกมาเป็นหลายไฟล์ เพราะใช้หลายเครื่องในการประมวลผล"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w6ZMEZ-8UIEJ"
      },
      "source": [
        "# เซฟเป็น partitioned files (ใช้ multiple workers)\n",
        "dt_clean.write.csv('Cleaned_data.csv', header = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mcPc7I-BYckx"
      },
      "source": [
        "เราสามารถบังคับให้ Spark เซฟมาเป็นไฟล์เดียวได้"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AzMterYlWA5X"
      },
      "source": [
        "# เซฟเป็น 1 ไฟล์ (ใช้ single worker)\n",
        "dt_clean.coalesce(1).write.csv('Cleaned_Data_Single.csv', header = True)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}